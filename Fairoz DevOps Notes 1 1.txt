---------------- Terraform ---------------------------------------
- terraform init
- terraform plan
- terraform apply

example :-
terraform init
terraform workspace select europe-west3_philips
terraform plan -var-file="./Terraform-Envs/europe-west3_philips/philips.tfvars"
terraform apply -var-file="./Terraform-Envs/europe-west3_philips/philips.tfvars"
terraform state mv 'google_cloudfunctions2_function.MigrateDelta' 'google_cloudfunctions2_function.functions[\"migrateDelta\"]'

----------------- Terraform deployment commands -----------------------------
terraform init
 
terraform workspace select us-east1_dev
 
terraform plan -var-file="./Terraform-Envs/us-east1_dev/dev.tfvars"
 
terraform apply -var-file="./Terraform-Envs/us-east1_dev/dev.tfvars"

terraform untaint module.s3_bucket.aws_s3_bucket.s3_bucket
----------------- Terraform GCP docs -----------------------------
https://cloud.google.com/docs/terraform/samples

----------------- Terraform import and state commands -----------------------------
terraform import aws_api_gateway_rest_api.smithy_api sfhca9dxzi
terraform state show aws_lambda_function.brightsmith_swagger
terraform import aws_lambda_function.brightsmith_swagger arn:aws:lambda:us-east-2:419572703193:function:brightsmith--swagger
terraform state rm aws_lambda_function.brightsmith_swagger
terraform import -var-file="prod.tfvars" google_cloud_run_v2_service.pdf_generator_service projects/metiss-prod/locations/us-west1/services/pdf-generator-service
terraform import -var-file="prod.tfvars" module.app.google_cloud_run_v2_service.v2_cloudrun projects/metiss-dev/locations/us-west1/services/widget-service
terraform import -var-file="stage.tfvars" module.app.google_storage_bucket.vista_bucket metiss-vista-web-stage
terraform import -var-file="prod.tfvars" module.app.google_artifact_registry_repository.container_repo us/gcr.io
terraform import -var-file="dev.tfvars" module.app.google_sql_database_instance.cloud_sql_db projects/metiss-dev/instances/metiss
terraform import -var-file="dev.tfvars" module.app.module.pdf_generator_service.google_cloud_run_service.main projects/metiss-dev/locations/us-west1/services/pdf-generator-service
terraform import "module.app.google_cloud_run_v2_service.v2_cloudrun[\`"solence_service\`"]" "projects/metiss-prod/locations/us-west1/services/solence"


state lock
terraform force-unlock
---------------- Ubuntu ------------------------------------------
how to check which ubuntu version :- run this command "lsb_release -a"

how to do SFTP set up - 
add or uncomment this line "Subsystem sftp /usr/lib/openssh/sftp-server" in this file - "/etc/ssh/sshd_config".
after that follow below documentation:-
https://www.digitalocean.com/community/tutorials/how-to-enable-sftp-without-shell-access-on-ubuntu-20-04

---------------------- Docker ----------------------------------------------------

Stop all the running container - docker stop $(docker ps -q)
remove all the images - docker rmi --force $(docker images -q)
remove all the container - docker rm -f $(docker ps -q)
docker system prune --volumes --force


------------------------- nohup command ----------------------------

- nohup stands for no hang up
- nohup is indeed a command-line utility or tool in Unix-like operating systems.
- It's commonly used to execute commands or scripts in a way that allows them to continue running even after the user who initiated the command logs out or the terminal session is terminated.
example
- nohup pnpm dev > resumebuilderfairoz-$(date -u +%Y%m%d%H).log 2>&1 &
- nohup: This is the command itself. It's used to execute another command in a way that allows it to continue running even after the terminal session is terminated.

- pnpm dev: This is the command or process that you want to run. In this case, it appears to be starting a development server using pnpm.

- >: This is a redirection operator. It's used to redirect the standard output of the command (pnpm dev) to a file instead of displaying it in the terminal.
- resumebuilderfairoz-$(date -u +%Y%m%d%H).log: This is the name of the file where the output of the command will be redirected. It includes a timestamp generated by the date command in the format YYYYMMDDHH, which represents the year, month, day, and hour (in 24-hour format) in Coordinated Universal Time (UTC). So, the filename will look something like resumebuilderfairoz-2024042705.log for the current date and hour.
- 2>&1: This is another redirection operator. It's used to redirect the standard error (file descriptor 2) to the same location as the standard output (file descriptor 1). So, any error messages produced by the command will also be redirected to the specified log file.
- &: This puts the entire command (nohup pnpm dev > resumebuilderfairoz-$(date -u +%Y%m%d%H).log 2>&1) in the background, allowing you to continue using the terminal while the command runs.

-------------------------------------- System configuration check linux -----------------------
- run "uname -m" command
- When you run this command, it will output a string indicating the machine hardware name. On an AMD64 (x86_64) system, it typically outputs x86_64, while on an ARM64 system, it typically outputs aarch64.

------------------------------------------------

ps aux | grep "command"

----------------------- DNS ---------------------------

A record - 

test the domain - test the domain - https://www.bing.com/ck/a?!&&p=a4dcb72874b5ca6cJmltdHM9MTcxNTA0MDAwMCZpZ3VpZD0wNWMxZWNiOS00NWY5LTY0NjEtMTVkZi1mZjIzNDQ1NjY1OWImaW5zaWQ9NTIxMw&ptn=3&ver=2&hsh=3&fclid=05c1ecb9-45f9-6461-15df-ff234456659b&psq=ssllabs+ssl+check&u=a1aHR0cHM6Ly93d3cuc3NsbGFicy5jb20vc3NsdGVzdC8&ntb=1


-------------------- Sonar Qube--------------------------------------

How to install Sonar qube in Windows machine and run it in background

1. Download the sonar qube from official website
2. open cmd and navigate to the sonarqube-10.5.1.90531\sonarqube-10.5.1.90531\bin\windows-x86-64 directory 
3. Run "SonarService.bat install" to Install the Service
4. Close the Command Prompt window you currently have open.
5. Right-click on the Command Prompt icon in the Start menu.
6. Select "Run as administrator" from the context menu.
7. When prompted for confirmation, click "Yes" to allow Command Prompt to run with administrative privileges.
8. Once Command Prompt opens with administrator privileges, navigate to the SonarQube installation directory using the cd command: "cd C:\Users\Fairoz\Documents\ANT\sonarqube-10.5.1.90531\sonarqube-10.5.1.90531\bin\windows-x86-64"
9. Now, try starting the SonarQube service again using the net start command: "net start SonarQube"
10. Go to services start the sonarqube 




---------------------------------------------------------

ssh-keygen -t rsa -b 4096 -C "fairoz.mohammed@symphonize.com"


------------------------------------------------


dig and nslookup is a command-line tool used to query DNS (Domain Name System) servers to obtain domain name or IP address mapping, DNS records, and other DNS-related information

example - dig hostname
example - nslookup hostname


-----------------------------------
terraform issue - https://chatgpt.com/share/cac2b236-36ca-4c56-86dc-bd1c50eec3ff


--------------------------------------

echo | openssl s_client -connect 10.84.89.15:8090 -servername 10.84.89.15 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > server-cert.crt

sudo cp server-cert.crt /etc/pki/ca-trust/source/anchors/



sudo keytool -import -trustcacerts -file /etc/pki/ca-trust/source/anchors/server-cert.crt -alias symphonize-cert -keystore /usr/lib/jvm/java-17-amazon-corretto/lib/security/cacerts -storepass changeit



1.mkdir -p /appInstall/docker_logs
2.rsync -av /var/lib/docker/volumes/bumblebee-colodev_egpsp-connector-service-colodev/_data/logs/ /appInstall/docker_logs/
3.crontab -e
4.*/5 * * * * rsync -av /var/lib/docker/volumes/bumblebee-colodev_egpsp-connector-service-colodev/_data/logs/ /appInstall/docker_logs/


C:\Users\Fairoz\Documents\Fairoz-tsks\sent-migrate\Sentinel-migrate-Us_east-Dev\sentinel-migrate\src\api\Terraform-Envs\us-east1_dev\execute_sql_script.js

-------------------------------------------------------------------------------------------

ec2 machine script

#!/bin/bash
# Use this for your user data
# install httpd
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)<h1>" > /var/www/html/index.html

----------------------------------------------------------------------------------------------


You might be referring to clearing temporary files or cache, which can sometimes improve performance by freeing up space. Here are some common locations in Windows that you can access via the **Run** dialog to delete unnecessary files:

1. **Temp folder**:
   - Press `Windows + R` to open the **Run** dialog.
   - Type `temp` and press **Enter**.
   - This will open the folder containing temporary files. You can select and delete these files. Some files in use may not be deletable.

2. **%Temp% folder**:
   - Press `Windows + R` to open the **Run** dialog.
   - Type `%temp%` and press **Enter**.
   - This opens the user-specific temporary files folder. You can safely delete most files here.

3. **Prefetch folder**:
   - Press `Windows + R` to open the **Run** dialog.
   - Type `prefetch` and press **Enter**.
   - This folder contains information to speed up the loading of applications. Deleting files here can free up space, but they will regenerate when needed.

4. **Disk Cleanup**:
   - Press `Windows + R` to open the **Run** dialog.
   - Type `cleanmgr` and press **Enter**.
   - Select the drive (usually **C:**) and let Windows calculate how much space you can free up. You can select categories like temporary files, system files, and more.

Deleting these files can help free up disk space, which might improve performance slightly, but it‚Äôs not guaranteed to make the computer faster in all cases. For more significant improvements, you may want to consider other steps, such as uninstalling unused programs or upgrading hardware (like RAM or SSD).


------------------------------------------------------------------------------------------------------

Networking

ip address:-
   - ipv4: - ex: 192.11.22.33
           - each block of ipv4 varies from 0 to 255
           - 0-255*0-255*0-255*0-255
           - 4 bytes and 32 bits(1 byte = 8 bits)
           - each byte is separated by a dot
           - how computer understands ipv4 address? - ex if ip is : 192.11.22.33 - 11000000.00001011.00010110.00001001
           - Step-by-Step Conversion
             üßÆ Convert 192 to Binary:
                  To convert 192 to binary, we break it down using powers of 2:

                        Power of 2	Value	Used?
                        2‚Å∑	128	‚úÖ Yes (192 ‚àí 128 = 64)
                        2‚Å∂	64	‚úÖ Yes (64 ‚àí 64 = 0)
                        2‚Åµ	32	‚ùå No
                        2‚Å¥	16	‚ùå No
                        2¬≥	8	‚ùå No
                        2¬≤	4	‚ùå No
                        2¬π	2	‚ùå No
                        2‚Å∞	1	‚ùå No
subnet:- 
   - private subnet:
       - 
   - public subnet:
       - 
CIDR:-

üî¢ Quick CIDR Table
CIDR	Subnet Mask	Host Bits	Usable IPs
/32	255.255.255.255	0	1 (single IP)
/30	255.255.255.252	2	2 usable
/29	255.255.255.248	3	6 usable
/28	255.255.255.240	4	14 usable
/24	255.255.255.0	   8	254 usable
/16	255.255.0.0	      16	65,534 usable
/8	   255.0.0.0	      24	16,777,214 usable

need 230 ip address
https://www.youtube.com/watch?v=iSOfkw_YyOU&t=490s

OSI model:-
   - Application Layer
   - Presentation Layer
   - Session Layer
   - Transport Layer
   - Network Layer
   - Data Link Layer
   - Physical Layer
----------------------------------------------------------------------------------------------

Elastic beanstalk deployment

1. All at once :-

¬∑ Fastest deployment
¬∑ Application has downtime
. Great for quick iterations in development environment
¬∑ No additional cost

v1 --> stop --> v2
v1 --> stop --> v2
v1 --> stop --> v2
v1 --> stop --> v2

2. Rolling :- 

¬∑ Application is running below capacity
¬∑ Can set the bucket size
¬∑ Application is running both versions simultaneously
¬∑ No additional cost
. Long deployment

v1 --> stop --> v2 --> v2
v1 --> stop --> v2 --> v2
v1 --> v1 --> v1 --> stop --> v2
v1 --> v1 --> v1 --> stop --> v2

3. Rolling with additional batches

¬∑ Application is running at capacity
¬∑ Can set the bucket size
¬∑ Application is running both versions simultaneously
¬∑ Small additional cost
¬∑ Additional batch is removed at the end of the deployment
¬∑ Longer deployment
. Good for prod

v1 --> v1 --> stop --> v2 --> v2 --> v2
v1 --> v1 --> stop --> v2 --> v2 --> v2
v1 --> v1 --> v1 --> v1 --> stop --> v2
v1 --> v1 --> v1 --> v1 --> stop --> v2
new --> v2 --> v2 --> v2 --> v2 --> v2 --> terminated
new --> v2 --> v2 --> v2 --> v2 --> v2 --> terminated

4. Elastic Beanstalk Deployment Immutable

¬∑ Zero downtime
. New Code is deployed to new instances on a temporary ASG
. High cost, double capacity
. Longest deployment
. Quick rollback in case of failures (just terminate new ASG)
. Great for prod

-----------------------------------------------------------------------------------------------------------------------
copy gcp bucket to local

gsutil -m cp -r gs://persimmon-solr-asia/data/resumes/data/* .

reverse local to gcp bucket

gsutil -m cp -r gs://persimmon-ui/* gs://persimmon-ui-stage/

--------------------
wsl --shutdown

---------------------
logs for user troubleshoot gcp

gcloud logging read   'resource.type="cloud_function" AND resource.labels.function_name="function-1"'   --limit 10 --format=json --freshness=90d

gcloud logging read   'resource.type="gce_instance"'   --limit 10 --format=json --freshness=18d | grep "user"

--------------------------------------------------------------------------------------------------------------------------------------

Push all your changes to your feature branch before doing the below three steps. So that all your changes are in your remote branch. Then, do rebasing with another branch (the main branch). The latest changes of  main branch are in your local branch, do a force push. After that, all the latest changes from the main branch will get to your remote branch.
 
1.git fetch --all
 2. git rebase -i origin/main
 3.git push --force-with-lease
 if you don't want to push local changes to repo
 
 
 ------------------------------------------------------------------------------
 ssl renewal
 
 openssl pkey -in /etc/letsencrypt/live/resume.bitlabs.in/privkey.pem -out /var/www/ResumeBuilder/ssl/resume.bitlabs.in.key
 
 openssl x509 -in /etc/letsencrypt/live/resume.bitlabs.in/fullchain.pem -out /var/www/ResumeBuilder/ssl/resume.bitlabs.in.crt
 
 
 ----------------------------------------------------------------------------------------------------------------
 Compress-Archive -Path "C:\Users\Fairoz\Documents\*" -DestinationPath "C:\Users\Fairoz\Documents\old-computer.zip"

 ----------------------------------------------------------------------------------------------------------------
nginx

sudo nano /etc/nginx/sites-available/my_secure_site.conf

example file :- 
server {
   listen 443 ssl;
   server_name example.com;  # No domain; this will catch all traffic on port 443

   # SSL certificate and key
   ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem;
   ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem;
   # SSL configurations
   ssl_protocols TLSv1.2 TLSv1.3;
   ssl_prefer_server_ciphers on;

   # Redirect all HTTP to HTTPS (if you're also using port 80)
   error_log /var/log/nginx/error.log;
   access_log /var/log/nginx/access.log;

   # SSL cipher settings
   ssl_ciphers 'TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:ECDHE-RSA-AES256-GCM-SHA384';
   ssl_dhparam /etc/ssl/certs/dhparam.pem;  # Optional: for better security, generate a Diffie-Hellman param

   # Backend service routing based on the path
   location /QA/ {
       proxy_pass http://127.0.0.1:8983/;
       proxy_set_header Host $host;
       proxy_set_header X-Real-IP $remote_addr;
       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
   }

   location /dev/ {
       proxy_pass http://127.0.0.1:8993/;
       proxy_set_header Host $host;
       proxy_set_header X-Real-IP $remote_addr;
       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
   }

   location /prod/ {
       proxy_pass http://127.0.0.1:8973/;
       proxy_set_header Host $host;
       proxy_set_header X-Real-IP $remote_addr;
       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
   }
}

# Optionally, set up HTTP redirect to HTTPS
server {
   listen 80;
   server_name example.com;  # Catch-all for HTTP requests

   return 301 https://$host$request_uri;
} 
 

commands:

----------------------------------------------------------
üîêSet Up SSH Key for GitHub Actions

On your local machine:
ssh-keygen -t rsa -b 4096 -C "github-deploy" -f lightsail_deploy_key
This gives you:

lightsail_deploy_key (private key)

lightsail_deploy_key.pub (public key)

On your Lightsail instance:
SSH into your instance from the browser or terminal:


ssh -i /path/to/key.pem ubuntu@<your-lightsail-ip>
Add the public key:


mkdir -p ~/.ssh
echo "ssh-rsa sqN1NvF8vmJfjsxBQinHaG/abc+8Q== github-deploy" >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys


-------------------------------------------------
git reset --hard c5bc292f5df69486526dff4cb53c9cdee8ca0f1d
 
 
just by running this command we can delete the commit right?

------------------------------------------------
terraform taint google_cloud_run_v2_service.backend_role_service
# terraform taint <resource_type>.<resource_name>
# Marks a Terraform-managed resource as "tainted", so it will be destroyed and recreated on the next apply.
# Useful for forcing recreation of a resource without changing configuration.

# Example:
terraform taint google_cloud_run_v2_service.backend_role_service

# To untaint (remove taint mark):
terraform untaint google_cloud_run_v2_service.backend_role_service

-------------------------------------------------
mount volume
sudo file -s /dev/xvdb1 - check the status of the volume to mount
sudo fsck /dev/xvdb1 - Run Filesystem Check and Repair You need to run fsck (filesystem check) before mounting:
sudo mkdir /mnt/oldvolume
sudo mount /dev/xvdb1 /mnt/oldvolume - mount the volume

--------------------------------------------------

3-tier aws vpc project

https://catalog.us-east-1.prod.workshops.aws/workshops/85cd2bb2-7f79-4e96-bdee-8078e469752a/en-US

------------------------------------------------------

Authenticate with the GCP SA Key

gcloud auth activate-service-account --key-file="C:\Users\Fairoz\Documents\Fairoz-task\metiss-dev-a316664ea2dd.json"


---------------------------------------------

kubernetes

communication between service
<service-name>.<namespace>.svc.cluster.local
--------------------------------------

CORS

tuple(scheme+Host+port)
https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS

---------------------------------------
AWS CLI commands

check a specific instance type across all AZs in us-east-1:

aws ec2 describe-instance-type-offerings --location-type availability-zone --filters Name=instance-type,Values=c7i.large --region us-east-1 --output table

ssh -i "k8s-nodes.pem" ubuntu@ipadress

setup multi node - github - https://github.com/piyushsachdeva/CKA-2024/tree/main/Resources/Day27


change the name of terminal
PS1="[fairoz@master \W]\\$ "

check private ip
hostname -I

sudo kubeadm reset -f

--------------------------

The hosts file in Windows is located at this path:

C:\Windows\System32\drivers\etc\hosts

-----------------------------------------

Linux commands

- check group exist - getent group docker

- Apply the new group membership immediately - "newgrp docker"
    newgrp: Command to switch the current session to a new group.
    docker: The group you're switching to.

- Check CPU Architecture Use - "uname -m"
